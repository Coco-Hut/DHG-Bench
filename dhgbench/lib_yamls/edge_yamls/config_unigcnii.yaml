default:
  All_num_layers: 2
  MLP_hidden: 256
  epochs: 50 # 62.82+-0.13
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.8

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.3
  lamda: 0.8

cora:
  All_num_layers: 2
  MLP_hidden: 256
  epochs: 50 # 62.82+-0.13
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.8

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.3
  lamda: 0.8

pubmed:
  All_num_layers: 2
  MLP_hidden: 128
  epochs: 100 # 62.82+-0.13
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.0

  input_drop: 0.3
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.5
  lamda: 0.5

coauthor_cora:
  All_num_layers: 2
  MLP_hidden: 256
  epochs: 50 # 62.82+-0.13
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.8

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.3
  lamda: 0.8

coauthor_dblp:
  All_num_layers: 2
  MLP_hidden: 128
  epochs: 100 
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.5

  input_drop: 0.3
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.2
  lamda: 0.3

yelp:
  All_num_layers: 2
  MLP_hidden: 128
  epochs: 180 
  lr: 1.0e-2
  wd: 0.0
  dropout: 0.2

  input_drop: 0.0
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.8
  lamda: 0.8

walmart-trips-100:
  All_num_layers: 2
  MLP_hidden: 128
  epochs: 200 
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.5

  input_drop: 0.3
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.0
  lamda: 0.3

actor:
  All_num_layers: 1
  MLP_hidden: 256
  epochs: 200 
  lr: 1.0e-2
  wd: 0.0
  dropout: 0.5

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 1.0
  lamda: 0.5

amazon:
  All_num_layers: 1
  MLP_hidden: 128
  epochs: 50 
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.1

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.0
  lamda: 0.8

twitch:
  All_num_layers: 1
  MLP_hidden: 128
  epochs: 100 
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.1

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.0
  lamda: 0.5

pokec:
  All_num_layers: 1
  MLP_hidden: 128
  epochs: 100 
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.0

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.5
  lamda: 0.5

german:
  All_num_layers: 1
  MLP_hidden: 128
  epochs: 200 
  lr: 1.0e-3
  wd: 0.0
  dropout: 0.5

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: True
  restart_alpha: 1.0
  lamda: 0.5

bail:
  All_num_layers: 1
  MLP_hidden: 256
  epochs: 40 
  lr: 1.0e-5
  wd: 0.0001
  dropout: 0.0

  input_drop: 0.0
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.1
  lamda: 0.8

credit:
  All_num_layers: 1
  MLP_hidden: 256
  epochs: 100
  lr: 1.0e-3
  wd: 0.0001
  dropout: 0.0

  input_drop: 0.0
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.5
  lamda: 0.8