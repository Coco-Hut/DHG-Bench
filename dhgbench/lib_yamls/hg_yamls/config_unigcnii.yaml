default:
  All_num_layers: 1
  embedding_hidden: 256
  MLP_hidden: 128
  epochs: 200
  dropout: 0.5
  lr: 1.0e-2
  wd: 0

  first_aggregate: "sum"   # first_aggregate: ['mean', 'sum']
  input_drop: 0.2
  attn_drop: 0.2
  activation: "relu" # activation: ['relu', 'prelu']
  use_norm: true
  uni_heads: 1
  restart_alpha: 0.5
  UniGNN_degV: 0
  UniGNN_degE: 0

RHG_10:
  All_num_layers: 2
  embedding_hidden: 64
  MLP_hidden: 64
  epochs: 200
  dropout: 0.2
  lr: 1.0e-2
  wd: 0
  pooling: "mean"
  early_stop: true

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.8
  lamda: 0.8

RHG_3:
  All_num_layers: 2
  embedding_hidden: 64
  MLP_hidden: 64
  epochs: 200
  dropout: 0.2
  lr: 1.0e-2
  wd: 0
  pooling: "mean"
  early_stop: true

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.8
  lamda: 0.8

IMDB_dir_form:
  All_num_layers: 1
  embedding_hidden: 64
  MLP_hidden: 64
  epochs: 200
  dropout: 0.2
  lr: 1.0e-2
  wd: 0
  pooling: "max"
  early_stop: true

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.8
  lamda: 0.8

IMDB_dir_genre:
  All_num_layers: 2
  embedding_hidden: 64
  MLP_hidden: 64
  epochs: 200
  dropout: 0.2
  lr: 1.0e-2
  wd: 0
  pooling: "mean"
  early_stop: true

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.1
  lamda: 0.8

stream_player:
  All_num_layers: 2
  embedding_hidden: 64
  MLP_hidden: 64
  epochs: 200
  dropout: 0.2
  lr: 1.0e-2
  wd: 0
  pooling: "mean"
  early_stop: true

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.1
  lamda: 0.8

twitter_friend:
  All_num_layers: 1
  embedding_hidden: 64
  MLP_hidden: 64
  epochs: 200
  dropout: 0.2
  lr: 1.0e-2
  wd: 0
  pooling: "max"
  early_stop: true

  input_drop: 0.5
  activation: "prelu" # activation: ['relu', 'prelu']
  use_norm: False
  restart_alpha: 0.8
  lamda: 0.8